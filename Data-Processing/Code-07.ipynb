{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN0i8ERUr0vQTX1hKI6bcmd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","# Mount Google Drive\n","drive.mount('/content/drive')"],"metadata":{"id":"cBLD0WRm6-lP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749807843050,"user_tz":-60,"elapsed":26800,"user":{"displayName":"Freelance Projet","userId":"06036806217603537811"}},"outputId":"16004567-354d-4c31-d1c3-e33ea7fecb48"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n","import joblib\n","import os\n","import pickle"],"metadata":{"id":"m93a1YLa7AxO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 1: Calculate ERT and RELERT\n","def load_data(file_path, sheet_name=0):\n","    \"\"\"Load data from an Excel file.\"\"\"\n","    if not os.path.exists(file_path):\n","        print(f\"Error: File {file_path} does not exist.\")\n","        return None\n","    try:\n","        data = pd.read_excel(file_path, sheet_name=sheet_name)\n","        data['Measured Fitness'] = pd.to_numeric(data['Measured Fitness'], errors='coerce')\n","        data['Fopt'] = pd.to_numeric(data['Fopt'], errors='coerce')\n","        data['Function Evaluations'] = pd.to_numeric(data['Function Evaluations'], errors='coerce')\n","        return data\n","    except Exception as e:\n","        print(f\"Error loading data: {e}\")\n","        return None"],"metadata":{"id":"jZ4c5qRY7CL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_ert(data, epsilon=1e-2):\n","    \"\"\"Calculate the Expected Runtime (ERT) using a specific success criterion.\"\"\"\n","    if data is None:\n","        return None\n","    try:\n","        data['success'] = data['Measured Fitness'] <= (data['Fopt'] + epsilon)\n","        ert_data = data.groupby(['Algorithm', 'Function', 'Instance', 'Dimension']).apply(\n","            lambda x: pd.Series({\n","                'ERT': x['Function Evaluations'].sum() / x['success'].sum() if x['success'].sum() > 0 else float('inf')\n","            }), include_groups=False\n","        ).reset_index()\n","        return ert_data\n","    except KeyError as e:\n","        print(f\"Missing column: {e}\")\n","        return None"],"metadata":{"id":"HpZxiiZj7Dvh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_relert(ert_data):\n","    \"\"\"Calculate the Relative Expected Runtime (RELERT).\"\"\"\n","    if ert_data is None:\n","        return None\n","    try:\n","        min_ert = ert_data.groupby(['Function', 'Dimension'])['ERT'].min().reset_index()\n","        min_ert.rename(columns={'ERT': 'min_ERT'}, inplace=True)\n","        merged_data = pd.merge(ert_data, min_ert, on=['Function', 'Dimension'])\n","        merged_data['RELERT'] = merged_data['ERT'] / merged_data['min_ERT']\n","        par10 = 1000000\n","        merged_data['RELERT'] = merged_data['RELERT'].replace([float('inf')], par10).fillna(par10)\n","        return merged_data\n","    except KeyError as e:\n","        print(f\"Missing column: {e}\")\n","        return None"],"metadata":{"id":"PF6k08J27GVL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process_data(file_path, sheet_name=0, epsilon=1e-2):\n","    data = load_data(file_path, sheet_name)\n","    if data is not None:\n","        ert_data = calculate_ert(data, epsilon)\n","        if ert_data is not None:\n","            relert_data = calculate_relert(ert_data)\n","            if relert_data is not None:\n","                output_path = file_path.replace('.xlsx', '_relert.csv')\n","                relert_data.to_csv(output_path, index=False)\n","                print(f\"RELERT results saved to {output_path}\")\n","                return relert_data\n","            else:\n","                print(\"Failed to calculate RELERT.\")\n","        else:\n","            print(\"Failed to calculate ERT.\")\n","    return None"],"metadata":{"id":"cY5LJb0E7InA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 2: Select Top 10 Algorithms\n","def select_top_algorithms(relert_data, top_n=11):\n","    \"\"\"Select top N algorithms based on mean ERT.\"\"\"\n","    if relert_data is None:\n","        return None\n","    try:\n","        # Replace infinite ERT values with a large number\n","        relert_data['ERT'] = relert_data['ERT'].replace([float('inf')], 1e6)\n","        # Calculate mean ERT per algorithm\n","        mean_ert = relert_data.groupby('Algorithm')['ERT'].mean().reset_index()\n","        # Select top N algorithms\n","        top_algorithms = mean_ert.sort_values('ERT').head(top_n)['Algorithm'].tolist()\n","        return top_algorithms\n","    except KeyError as e:\n","        print(f\"Error: Missing column {e}\")\n","        return None"],"metadata":{"id":"4qpNYGNS7Kdu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def filter_top_algorithms_data(relert_data, top_algorithms):\n","    \"\"\"Filter data for top algorithms.\"\"\"\n","    if relert_data is None or top_algorithms is None:\n","        return None\n","    try:\n","        filtered_data = relert_data[relert_data['Algorithm'].isin(top_algorithms)]\n","        output_path = '/content/drive/MyDrive/Wail-Projet-F/Data-04/top_10_algorithms_data.csv'\n","        filtered_data.to_csv(output_path, index=False)\n","        print(f\"Filtered data for top 10 algorithms saved to {output_path}\")\n","        return output_path\n","    except KeyError as e:\n","        print(f\"Error: Missing column {e}\")\n","        return None"],"metadata":{"id":"0APmx3Sy7MN_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 3: Identify Best Algorithms for Top 10\n","def label_best_algorithms(relert_file):\n","    if not os.path.exists(relert_file):\n","        print(f\"Error: File {relert_file} does not exist.\")\n","        return None\n","    try:\n","        ert_results = pd.read_csv(relert_file)\n","        idx = ert_results.groupby(['Function', 'Instance', 'Dimension'])['RELERT'].idxmin()\n","        best_algorithms = ert_results.loc[idx]\n","        best_algorithms.rename(columns={'Algorithm': 'Best Algorithm'}, inplace=True)\n","        final_data = pd.merge(ert_results, best_algorithms[['Function', 'Instance', 'Dimension', 'Best Algorithm']],\n","                              on=['Function', 'Instance', 'Dimension'], how='left')\n","        final_data_csv_path = '/content/drive/MyDrive/Wail-Projet-F/Data-04/labeledfeatures_top10.csv'\n","        final_data.to_csv(final_data_csv_path, index=False)\n","        print(\"Final labeled data saved to:\", final_data_csv_path)\n","        return final_data\n","    except KeyError as e:\n","        print(f\"Error: {e} - Check column names and ensure they are correct.\")\n","        return None"],"metadata":{"id":"idgemHFS7OPu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 4: Merge with Features and Filter\n","def merge_and_filter(features_file, labeled_file):\n","    if not os.path.exists(features_file):\n","        print(f\"Error: File {features_file} does not exist. Skipping merge_and_filter.\")\n","        return None\n","    if not os.path.exists(labeled_file):\n","        print(f\"Error: File {labeled_file} does not exist. Skipping merge_and_filter.\")\n","        return None\n","    try:\n","        features_data = pd.read_excel(features_file)\n","        labeled_performance_data = pd.read_csv(labeled_file)\n","        print(\"Features Data Columns:\", features_data.columns.tolist())\n","        print(\"Labeled Performance Data Columns:\", labeled_performance_data.columns.tolist())\n","        merged_data = pd.merge(features_data, labeled_performance_data, left_on=['FID', 'IID', 'Dimension'],\n","                               right_on=['Function', 'Instance', 'Dimension'])\n","        merged_data.to_csv('/content/drive/MyDrive/Wail-Projet-F/Data-04/merged_dataset_top10.csv', index=False)\n","        print(\"Merged data saved to 'merged_dataset_top11.csv'.\")\n","\n","        data = pd.read_csv('/content/drive/MyDrive/Wail-Projet-F/Data-04/merged_dataset_top10.csv')\n","        data.replace([np.inf, -np.inf], np.nan, inplace=True)\n","        numeric_cols = data.select_dtypes(include=[np.number]).columns\n","        column_medians = data[numeric_cols].median()\n","        data[numeric_cols] = data[numeric_cols].fillna(column_medians)\n","        filtered_df = data[data['Algorithm'] == data['Best Algorithm']]\n","        final_df = filtered_df.drop(columns=['Algorithm'])\n","        final_df.to_csv('/content/drive/MyDrive/Wail-Projet-F/Data-04/final_filtered_dataset_top10.csv', index=False)\n","        print(\"Final filtered dataset saved with\", len(final_df), \"rows.\")\n","        return final_df\n","    except Exception as e:\n","        print(f\"Error in merge_and_filter: {e}\")\n","        return None"],"metadata":{"id":"tta5TvXm7Q4W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 5: Normalize and Encode\n","def normalize_and_encode(train_file):\n","    if not os.path.exists(train_file):\n","        print(f\"Error: File {train_file} does not exist. Skipping normalize_and_encode.\")\n","        return None\n","    try:\n","        train_data = pd.read_csv(train_file)\n","\n","        columns_to_remove = [\n","            'bt.near.attractor_dists.sd',\n","            'bt.near.basin_intersection.sd',\n","            'gcm.near.basin_certain.sd',\n","            'gcm.near.basin_prob.sd',\n","            'gcm.near.basin_uncertain.sd'\n","        ]\n","        train_data = train_data.drop(columns=[col for col in columns_to_remove if col in train_data.columns])\n","\n","        train_features = train_data.drop(columns=['Best Algorithm'])\n","        train_target = train_data['Best Algorithm']\n","\n","        train_features = train_features.clip(-1e6, 1e6)\n","\n","        scaler = StandardScaler()\n","        train_features_scaled = scaler.fit_transform(train_features)\n","\n","        normalizer = MinMaxScaler()\n","        train_features_normalized = normalizer.fit_transform(train_features_scaled)\n","\n","        train_features_normalized = np.nan_to_num(train_features_normalized, nan=0.0, posinf=0.0, neginf=0.0)\n","\n","        with open('/content/drive/MyDrive/Wail-Projet-F/Data-04/scalerc_top10.pkl', 'wb') as f:\n","            pickle.dump(scaler, f)\n","        with open('/content/drive/MyDrive/Wail-Projet-F/Data-04/normalizerc_top10.pkl', 'wb') as f:\n","            pickle.dump(normalizer, f)\n","\n","        train_features_normalized_df = pd.DataFrame(train_features_normalized, columns=train_features.columns)\n","\n","        train_data_normalized = train_features_normalized_df.copy()\n","        train_data_normalized['Best Algorithm'] = train_target.values\n","\n","        le = LabelEncoder()\n","        train_data_normalized['Best Algorithm'] = le.fit_transform(train_data_normalized['Best Algorithm'])\n","        joblib.dump(le, '/content/drive/MyDrive/Wail-Projet-F/Data-04/label_encoderc_top10.pkl')\n","\n","        train_data_normalized.to_csv('/content/drive/MyDrive/Wail-Projet-F/Data-04/normalized_datasetc_top10.csv', index=False)\n","\n","        print(\"Normalized train dataset saved with\", len(train_data_normalized), \"rows.\")\n","        print(\"Unique classes in train:\", train_data_normalized['Best Algorithm'].unique())\n","        print(\"Class mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n","        return train_data_normalized\n","    except Exception as e:\n","        print(f\"Error in normalize_and_encode: {e}\")\n","        return None"],"metadata":{"id":"FQpEJkBO7TkY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_1hV8uEZr46","executionInfo":{"status":"ok","timestamp":1749807983875,"user_tz":-60,"elapsed":131880,"user":{"displayName":"Freelance Projet","userId":"06036806217603537811"}},"outputId":"67a8761e-fcc4-491e-e16f-407f2e3858fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["RELERT results saved to /content/drive/MyDrive/Wail-Projet-F/Data-04/optimization_data_relert.csv\n","Top 11 Algorithms by Mean ERT: ['CMAES-APOP-KMA_Nguyen', 'DE-BFGS_voglis_noiseless', 'ad-CMA-ES_Gissler', 'adm-CMA-ES_Gissler', 's-CMA-ES_Gissler', 'dm-CMA-ES_Gissler', 'a-CMA-ES', 'a-CMA-ES_Gissler', 'sd-CMA-ES_Gissler', 'BIPOP-CMA-ES', 'CMA-CSA_Atamna']\n","Filtered data for top 11 algorithms saved to /content/drive/MyDrive/Wail-Projet-F/Data-04/top_11_algorithms_data.csv\n","Final labeled data saved to: /content/drive/MyDrive/Wail-Projet-F/Data-04/labeledfeatures_top11.csv\n","Features Data Columns: ['FID', 'IID', 'Dimension', 'basic.blocks_max', 'basic.blocks_min', 'basic.cells_filled', 'basic.cells_total', 'basic.costs_fun_evals', 'basic.costs_runtime', 'basic.dim', 'basic.lower_max', 'basic.lower_min', 'basic.minimize_fun', 'basic.objective_max', 'basic.objective_min', 'basic.observations', 'basic.upper_max', 'basic.upper_min', 'bt.mean.attractor_dists.max', 'bt.mean.attractor_dists.mean', 'bt.mean.attractor_dists.median', 'bt.mean.attractor_dists.min', 'bt.mean.basin_intersection.max', 'bt.mean.basin_intersection.mean', 'bt.mean.basin_intersection.median', 'bt.mean.basin_intersection.min', 'bt.mean.basin_range', 'bt.mean.basin_ratio.certain', 'bt.mean.basin_ratio.most_likely', 'bt.mean.basin_ratio.uncertain', 'bt.mean.costs_fun_evals', 'bt.mean.costs_runtime', 'bt.mean.depth', 'bt.mean.leaves', 'bt.mean.levels', 'bt.mean.levels_nodes_ratio', 'bt.min.attractor_dists.max', 'bt.min.attractor_dists.mean', 'bt.min.attractor_dists.median', 'bt.min.attractor_dists.min', 'bt.min.basin_intersection.max', 'bt.min.basin_intersection.mean', 'bt.min.basin_intersection.median', 'bt.min.basin_intersection.min', 'bt.min.basin_range', 'bt.min.basin_ratio.certain', 'bt.min.basin_ratio.most_likely', 'bt.min.basin_ratio.uncertain', 'bt.min.costs_fun_evals', 'bt.min.costs_runtime', 'bt.min.depth', 'bt.min.leaves', 'bt.min.levels', 'bt.min.levels_nodes_ratio', 'bt.near.attractor_dists.max', 'bt.near.attractor_dists.mean', 'bt.near.attractor_dists.median', 'bt.near.attractor_dists.min', 'bt.near.attractor_dists.sd', 'bt.near.basin_intersection.max', 'bt.near.basin_intersection.mean', 'bt.near.basin_intersection.median', 'bt.near.basin_intersection.min', 'bt.near.basin_intersection.sd', 'bt.near.basin_range', 'bt.near.basin_ratio.certain', 'bt.near.basin_ratio.most_likely', 'bt.near.basin_ratio.uncertain', 'bt.near.costs_fun_evals', 'bt.near.costs_runtime', 'bt.near.depth', 'bt.near.leaves', 'bt.near.levels', 'bt.near.levels_nodes_ratio', 'cm_angle.angle.mean', 'cm_angle.angle.sd', 'cm_angle.costs_fun_evals', 'cm_angle.costs_runtime', 'cm_angle.dist_ctr2best.mean', 'cm_angle.dist_ctr2best.sd', 'cm_angle.dist_ctr2worst.mean', 'cm_angle.dist_ctr2worst.sd', 'cm_angle.y_ratio_best2worst.mean', 'cm_angle.y_ratio_best2worst.sd', 'cm_grad.costs_fun_evals', 'cm_grad.costs_runtime', 'cm_grad.mean', 'cm_grad.sd', 'disp.costs_fun_evals', 'disp.costs_runtime', 'disp.diff_mean_02', 'disp.diff_mean_05', 'disp.diff_mean_10', 'disp.diff_mean_25', 'disp.diff_median_02', 'disp.diff_median_05', 'disp.diff_median_10', 'disp.diff_median_25', 'disp.ratio_mean_02', 'disp.ratio_mean_05', 'disp.ratio_mean_10', 'disp.ratio_mean_25', 'disp.ratio_median_02', 'disp.ratio_median_05', 'disp.ratio_median_10', 'disp.ratio_median_25', 'ela_conv.conv_prob', 'ela_conv.costs_fun_evals', 'ela_conv.costs_runtime', 'ela_conv.lin_dev.abs', 'ela_conv.lin_dev.orig', 'ela_conv.lin_prob', 'ela_curv.costs_fun_evals', 'ela_curv.costs_runtime', 'ela_curv.grad_norm.lq', 'ela_curv.grad_norm.max', 'ela_curv.grad_norm.mean', 'ela_curv.grad_norm.med', 'ela_curv.grad_norm.min', 'ela_curv.grad_norm.nas', 'ela_curv.grad_norm.sd', 'ela_curv.grad_norm.uq', 'ela_curv.grad_scale.lq', 'ela_curv.grad_scale.max', 'ela_curv.grad_scale.mean', 'ela_curv.grad_scale.med', 'ela_curv.grad_scale.min', 'ela_curv.grad_scale.nas', 'ela_curv.grad_scale.sd', 'ela_curv.grad_scale.uq', 'ela_curv.hessian_cond.lq', 'ela_curv.hessian_cond.max', 'ela_curv.hessian_cond.mean', 'ela_curv.hessian_cond.med', 'ela_curv.hessian_cond.min', 'ela_curv.hessian_cond.nas', 'ela_curv.hessian_cond.sd', 'ela_curv.hessian_cond.uq', 'ela_distr.costs_fun_evals', 'ela_distr.costs_runtime', 'ela_distr.kurtosis', 'ela_distr.number_of_peaks', 'ela_distr.skewness', 'ela_level.costs_fun_evals', 'ela_level.costs_runtime', 'ela_level.lda_mda_10', 'ela_level.lda_mda_25', 'ela_level.lda_mda_50', 'ela_level.lda_qda_10', 'ela_level.lda_qda_25', 'ela_level.lda_qda_50', 'ela_level.mmce_lda_10', 'ela_level.mmce_lda_25', 'ela_level.mmce_lda_50', 'ela_level.mmce_mda_10', 'ela_level.mmce_mda_25', 'ela_level.mmce_mda_50', 'ela_level.mmce_qda_10', 'ela_level.mmce_qda_25', 'ela_level.mmce_qda_50', 'ela_level.qda_mda_10', 'ela_level.qda_mda_25', 'ela_level.qda_mda_50', 'ela_local.basin_sizes.avg_best', 'ela_local.basin_sizes.avg_non_best', 'ela_local.basin_sizes.avg_worst', 'ela_local.best2mean_contr.orig', 'ela_local.best2mean_contr.ratio', 'ela_local.costs_fun_evals', 'ela_local.costs_runtime', 'ela_local.fun_evals.lq', 'ela_local.fun_evals.max', 'ela_local.fun_evals.mean', 'ela_local.fun_evals.median', 'ela_local.fun_evals.min', 'ela_local.fun_evals.sd', 'ela_local.fun_evals.uq', 'ela_local.n_loc_opt.abs', 'ela_local.n_loc_opt.rel', 'ela_meta.costs_fun_evals', 'ela_meta.costs_runtime', 'ela_meta.lin_simple.adj_r2', 'ela_meta.lin_simple.coef.max', 'ela_meta.lin_simple.coef.max_by_min', 'ela_meta.lin_simple.coef.min', 'ela_meta.lin_simple.intercept', 'ela_meta.lin_w_interact.adj_r2', 'ela_meta.quad_simple.adj_r2', 'ela_meta.quad_simple.cond', 'ela_meta.quad_w_interact.adj_r2', 'gcm.mean.attractors', 'gcm.mean.basin_certain.max', 'gcm.mean.basin_certain.mean', 'gcm.mean.basin_certain.median', 'gcm.mean.basin_certain.min', 'gcm.mean.basin_certain.sum', 'gcm.mean.basin_prob.max', 'gcm.mean.basin_prob.mean', 'gcm.mean.basin_prob.median', 'gcm.mean.basin_prob.min', 'gcm.mean.basin_uncertain.max', 'gcm.mean.basin_uncertain.mean', 'gcm.mean.basin_uncertain.median', 'gcm.mean.basin_uncertain.min', 'gcm.mean.basin_uncertain.sum', 'gcm.mean.best_attr.no', 'gcm.mean.best_attr.prob', 'gcm.mean.costs_fun_evals', 'gcm.mean.costs_runtime', 'gcm.mean.pcells', 'gcm.mean.tcells', 'gcm.mean.uncertain', 'gcm.min.attractors', 'gcm.min.basin_certain.max', 'gcm.min.basin_certain.mean', 'gcm.min.basin_certain.median', 'gcm.min.basin_certain.min', 'gcm.min.basin_certain.sum', 'gcm.min.basin_prob.max', 'gcm.min.basin_prob.mean', 'gcm.min.basin_prob.median', 'gcm.min.basin_prob.min', 'gcm.min.basin_uncertain.max', 'gcm.min.basin_uncertain.mean', 'gcm.min.basin_uncertain.median', 'gcm.min.basin_uncertain.min', 'gcm.min.basin_uncertain.sum', 'gcm.min.best_attr.no', 'gcm.min.best_attr.prob', 'gcm.min.costs_fun_evals', 'gcm.min.costs_runtime', 'gcm.min.pcells', 'gcm.min.tcells', 'gcm.min.uncertain', 'gcm.near.attractors', 'gcm.near.basin_certain.max', 'gcm.near.basin_certain.mean', 'gcm.near.basin_certain.median', 'gcm.near.basin_certain.min', 'gcm.near.basin_certain.sd', 'gcm.near.basin_certain.sum', 'gcm.near.basin_prob.max', 'gcm.near.basin_prob.mean', 'gcm.near.basin_prob.median', 'gcm.near.basin_prob.min', 'gcm.near.basin_prob.sd', 'gcm.near.basin_uncertain.max', 'gcm.near.basin_uncertain.mean', 'gcm.near.basin_uncertain.median', 'gcm.near.basin_uncertain.min', 'gcm.near.basin_uncertain.sd', 'gcm.near.basin_uncertain.sum', 'gcm.near.best_attr.no', 'gcm.near.best_attr.prob', 'gcm.near.costs_fun_evals', 'gcm.near.costs_runtime', 'gcm.near.pcells', 'gcm.near.tcells', 'gcm.near.uncertain', 'ic.costs_fun_evals', 'ic.costs_runtime', 'ic.eps.max', 'ic.eps.ratio', 'ic.eps.s', 'ic.h.max', 'ic.m0', 'limo.avg_length.norm', 'limo.avg_length.reg', 'limo.cor.norm', 'limo.cor.reg', 'limo.costs_fun_evals', 'limo.costs_runtime', 'limo.length.mean', 'limo.length.sd', 'limo.ratio.mean', 'limo.ratio.sd', 'limo.sd_mean.norm', 'limo.sd_mean.reg', 'limo.sd_ratio.norm', 'limo.sd_ratio.reg', 'nbc.costs_fun_evals', 'nbc.costs_runtime', 'nbc.dist_ratio.coeff_var', 'nbc.nb_fitness.cor', 'nbc.nn_nb.cor', 'nbc.nn_nb.mean_ratio', 'nbc.nn_nb.sd_ratio', 'pca.costs_fun_evals', 'pca.costs_runtime', 'pca.expl_var.cor_init', 'pca.expl_var.cor_x', 'pca.expl_var.cov_init', 'pca.expl_var.cov_x', 'pca.expl_var_PC1.cor_init', 'pca.expl_var_PC1.cor_x', 'pca.expl_var_PC1.cov_init', 'pca.expl_var_PC1.cov_x']\n","Labeled Performance Data Columns: ['Algorithm', 'Function', 'Instance', 'Dimension', 'ERT', 'min_ERT', 'RELERT', 'Best Algorithm']\n","Merged data saved to 'merged_dataset_top11.csv'.\n","Final filtered dataset saved with 480 rows.\n","Normalized train dataset saved with 480 rows.\n","Unique classes in train: [2 3 6 4 5 9 1 0 7 8]\n","Class mapping: {'BIPOP-CMA-ES': np.int64(0), 'CMA-CSA_Atamna': np.int64(1), 'CMAES-APOP-KMA_Nguyen': np.int64(2), 'DE-BFGS_voglis_noiseless': np.int64(3), 'a-CMA-ES': np.int64(4), 'ad-CMA-ES_Gissler': np.int64(5), 'adm-CMA-ES_Gissler': np.int64(6), 'dm-CMA-ES_Gissler': np.int64(7), 's-CMA-ES_Gissler': np.int64(8), 'sd-CMA-ES_Gissler': np.int64(9)}\n"]}],"source":["# Main execution\n","if __name__ == \"__main__\":\n","    optimization_file = '/content/drive/MyDrive/Wail-Projet-F/Data-04/optimization_data.xlsx'\n","    features_file = '/content/drive/MyDrive/Wail-Projet-F/Data-04/features.xlsx'\n","\n","    relert_data = process_data(optimization_file, sheet_name=0, epsilon=1e-2)\n","    if relert_data is not None:\n","        top_algorithms = select_top_algorithms(relert_data, top_n=11)\n","        if top_algorithms is not None:\n","            print(f\"Top 11 Algorithms by Mean ERT: {top_algorithms}\")\n","            top_11_file = filter_top_algorithms_data(relert_data, top_algorithms)\n","            if top_11_file is not None:\n","                labeled_data = label_best_algorithms(top_11_file)\n","                if labeled_data is not None:\n","                    filtered_data = merge_and_filter(features_file, '/content/drive/MyDrive/Wail-Projet-F/Data-04/labeledfeatures_top10.csv')\n","                    if filtered_data is not None:\n","                        normalize_and_encode('/content/drive/MyDrive/Wail-Projet-F/Data-04/final_filtered_dataset_top10.csv')\n","                    else:\n","                        print(\"Skipping normalize_and_encode due to missing filtered data.\")\n","                else:\n","                    print(\"Skipping merge and normalization due to missing labeled data.\")\n","            else:\n","                print(\"Skipping labeling, merge, and normalization due to missing top 11 data.\")\n","        else:\n","            print(\"Skipping filtering, labeling, merge, and normalization due to failure in selecting top algorithms.\")\n","    else:\n","        print(\"Skipping all steps due to failure in ERT/RELERT calculation.\")"]},{"cell_type":"code","source":[],"metadata":{"id":"SPN7kClWZvLY"},"execution_count":null,"outputs":[]}]}